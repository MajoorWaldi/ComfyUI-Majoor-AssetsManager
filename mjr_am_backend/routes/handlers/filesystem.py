"""
Filesystem operations: background scanning and file listing.
"""

from __future__ import annotations

import asyncio
import datetime
import time
from collections import OrderedDict
from pathlib import Path
from typing import Any, Mapping, Optional

from mjr_am_backend.shared import Result, classify_file, get_logger, sanitize_error_message
from mjr_am_backend.config import (
    FS_LIST_CACHE_MAX,
    FS_LIST_CACHE_TTL_SECONDS,
    BG_SCAN_FAILURE_HISTORY_MAX,
    SCAN_PENDING_MAX,
    MANUAL_BG_SCAN_GRACE_SECONDS,
    BG_SCAN_ON_LIST,
    BG_SCAN_MIN_INTERVAL_SECONDS,
)
from mjr_am_backend.adapters.fs.list_cache_watcher import (
    ensure_fs_list_cache_watching,
    get_fs_list_cache_token,
)
from mjr_am_shared.scan_throttle import normalize_scan_directory, should_skip_background_scan
from .db_maintenance import is_db_maintenance_active
from ..core import _safe_rel_path, _is_within_root, _require_services

logger = get_logger(__name__)
VALID_SORT_KEYS = {"mtime_desc", "mtime_asc", "name_asc", "name_desc"}


def _normalize_sort_key(sort: Optional[str]) -> str:
    s = str(sort or "").strip().lower()
    if s in VALID_SORT_KEYS:
        return s
    return "mtime_desc"

# Locks for async concurrency safety
_BACKGROUND_SCAN_LOCK = asyncio.Lock()
_BACKGROUND_SCAN_LAST: OrderedDict[str, dict[str, Any]] = OrderedDict()
_BACKGROUND_SCAN_FAILURES: list[dict[str, Any]] = []
_MAX_FAILURE_HISTORY = int(BG_SCAN_FAILURE_HISTORY_MAX)
_BACKGROUND_SCAN_HISTORY_MAX = 1000

_SCAN_PENDING_LOCK = asyncio.Lock()
_SCAN_PENDING: OrderedDict[str, dict[str, Any]] = OrderedDict()
_SCAN_TASK: Optional[asyncio.Task[Any]] = None
_SCAN_PENDING_MAX = int(SCAN_PENDING_MAX)

_FS_LIST_CACHE_LOCK = asyncio.Lock()
_FS_LIST_CACHE: OrderedDict[str, dict[str, Any]] = OrderedDict()


def _is_truthy_boolish(value: Any) -> bool:
    if value is True:
        return True
    if value is False or value is None:
        return False
    if value in (1, "1"):
        return True
    if value in (0, "0"):
        return False
    return False


def _normalize_extension(value: Any) -> str:
    if value is None:
        return ""
    try:
        text = str(value).strip()
    except Exception:
        return ""
    if not text:
        return ""
    text = text.lstrip(".").strip(",;")
    return text.lower()


def _normalize_extensions(raw_list: Optional[Any]) -> list[str]:
    if not isinstance(raw_list, list):
        return []
    normalized: list[str] = []
    for item in raw_list:
        ext = _normalize_extension(item)
        if ext and ext not in normalized:
            normalized.append(ext)
    return normalized


async def _kickoff_background_scan(
    directory: str,
    *,
    source: str = "output",
    root_id: Optional[str] = None,
    recursive: bool = True,
    incremental: bool = True,
    fast: bool = True,
    background_metadata: bool = True,
    min_interval_seconds: float = float(BG_SCAN_MIN_INTERVAL_SECONDS),
) -> None:
    """
    Fire-and-forget indexing into the DB.

    Used to ensure input/custom folders get indexed without blocking list requests.
    """
    if not BG_SCAN_ON_LIST:
        return
    if is_db_maintenance_active():
        return
    normalized_dir = normalize_scan_directory(directory)
    if should_skip_background_scan(normalized_dir, source, root_id, MANUAL_BG_SCAN_GRACE_SECONDS):
        return

    try:
        key = f"{source}|{str(root_id or '')}|{normalized_dir}"
        now = time.time()

        def _peek_entry(k: str) -> tuple[float, bool]:
            try:
                e = _BACKGROUND_SCAN_LAST.get(k)
                if isinstance(e, dict):
                    last_v = float(e.get("last") or 0.0)
                    in_prog = bool(e.get("in_progress"))
                    return last_v, in_prog
                if isinstance(e, (int, float)):
                    return float(e), False
            except Exception:
                pass
            return 0.0, False
        
        last_seen, in_progress = _peek_entry(key)
        if in_progress:
            return
        if now - last_seen < min_interval_seconds:
            return

        async with _BACKGROUND_SCAN_LOCK:
            # Double-check under lock
            last_seen, in_progress = _peek_entry(key)
            if in_progress:
                return
            if now - last_seen < min_interval_seconds:
                return

        job = {
            "directory": str(normalized_dir),
            "source": str(source),
            "root_id": str(root_id) if root_id is not None else None,
            "recursive": bool(recursive),
            "incremental": bool(incremental),
            "fast": bool(fast),
            "background_metadata": bool(background_metadata),
        }

        _ensure_worker()

        async with _SCAN_PENDING_LOCK:
            if key in _SCAN_PENDING:
                return
            # Move to end if exists (refresh priority)
            _SCAN_PENDING[key] = job
            
            # Enforce max pending size
            while len(_SCAN_PENDING) > _SCAN_PENDING_MAX:
                _SCAN_PENDING.popitem(last=False)

        # Only mark as enqueued once we've successfully updated _SCAN_PENDING.
        _set_background_entry(key, {"last": now, "in_progress": False})
    except Exception:
        return


def _ensure_worker() -> None:
    global _SCAN_TASK
    if _SCAN_TASK and not _SCAN_TASK.done():
        return
    
    _SCAN_TASK = asyncio.create_task(_worker_loop())


def _set_background_entry(key: str, value: dict[str, Any]) -> None:
    _BACKGROUND_SCAN_LAST[key] = value
    try:
        _BACKGROUND_SCAN_LAST.move_to_end(key)
    except Exception:
        pass
    while len(_BACKGROUND_SCAN_LAST) > _BACKGROUND_SCAN_HISTORY_MAX:
        _BACKGROUND_SCAN_LAST.popitem(last=False)


async def _worker_loop() -> None:
    while True:
        task = None
        async with _SCAN_PENDING_LOCK:
            if _SCAN_PENDING:
                _, task = _SCAN_PENDING.popitem(last=False)
            else:
                task = None
        
        if not task:
            await asyncio.sleep(0.5)
            # Check exit condition or sleep again
            # We keep running to allow new tasks to arrive
            continue
        if is_db_maintenance_active():
            await asyncio.sleep(0.5)
            continue

        entry: Optional[dict[str, Any]] = None
        try:
            key = _build_task_key(task)
            entry = _ensure_background_entry(key)
            entry["in_progress"] = True
            entry["last"] = time.time()

            # Import strictly locally or use the core helper to avoid circular imports if possible
            # But here we are effectively inside backend.routes.handlers
            # We can use the helper from core.
            svc, error_result = await _require_services()
            if error_result:
                _record_scan_failure(str(task.get("directory")), str(task.get("source")), "SERVICE_UNAVAILABLE", str(error_result.error or ""))
                await asyncio.sleep(2) # Backoff if services unavailable
                continue
            if svc is None:
                _record_scan_failure(str(task.get("directory")), str(task.get("source")), "SERVICE_UNAVAILABLE", "Services are unavailable")
                await asyncio.sleep(2)
                continue

            try:
                try:
                    await svc["index"].scan_directory(
                        str(task.get("directory")),
                        bool(task.get("recursive")),
                        bool(task.get("incremental")),
                        source=str(task.get("source") or "output"),
                        root_id=task.get("root_id"),
                        fast=bool(task.get("fast")),
                        background_metadata=bool(task.get("background_metadata")),
                    )
                except TypeError:
                     # Fallback in case of call signature mismatch
                    await svc["index"].scan_directory(
                        str(task.get("directory")),
                        bool(task.get("recursive")),
                        bool(task.get("incremental")),
                        source=str(task.get("source") or "output"),
                        root_id=task.get("root_id"),
                    )
            except Exception as exc:
                _record_scan_failure(str(task.get("directory")), str(task.get("source")), "SCAN_FAILED", str(exc))
                logger.warning("Background scan failed: %s", exc)
                
            except Exception:
                try:
                    _record_scan_failure(str(task.get("directory")), str(task.get("source")), "EXCEPTION", "Unhandled exception")
                except Exception:
                    pass
        finally:
            if entry is not None:
                entry["in_progress"] = False
                entry["last"] = time.time()
        # Yield to event loop
        await asyncio.sleep(0)


def _record_scan_failure(directory: str, source: str, code: str, error: str) -> None:
    # No async lock needed for simple append/pop in main thread
    _BACKGROUND_SCAN_FAILURES.insert(0, {
        "timestamp": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        "directory": directory,
        "source": source,
        "code": code,
        "error": error
    })
    # Trim
    while len(_BACKGROUND_SCAN_FAILURES) > _MAX_FAILURE_HISTORY:
        _BACKGROUND_SCAN_FAILURES.pop()


def _ensure_background_entry(key: str) -> dict[str, Any]:
    entry = _BACKGROUND_SCAN_LAST.get(key)
    if isinstance(entry, dict):
        _set_background_entry(key, entry)
        return entry
    if isinstance(entry, (int, float)):
        entry = {"last": float(entry), "in_progress": False}
    else:
        entry = {"last": 0.0, "in_progress": False}
    _set_background_entry(key, entry)
    return entry


def _build_task_key(task: dict[str, Any]) -> str:
    source = str(task.get("source") or "output")
    root_id = str(task.get("root_id") or "")
    directory = normalize_scan_directory(str(task.get("directory") or ""))
    return f"{source}|{root_id}|{directory}"


def _collect_filesystem_entries(
    target_dir_resolved: Path,
    base: Path,
    asset_type: str,
    root_id: Optional[str],
) -> list[dict[str, Any]]:
    entries: list[dict[str, Any]] = []
    for entry in target_dir_resolved.iterdir():
        if not entry.is_file():
            continue
        filename = entry.name
        kind = classify_file(filename)
        if kind == "unknown":
            continue
        try:
            stat = entry.stat()
        except OSError:
            continue

        try:
            rel_to_root = entry.parent.relative_to(base)
            sub = "" if str(rel_to_root) == "." else str(rel_to_root).replace("\\", "/")
        except ValueError:
            sub = ""
        entries.append(
            {
                "id": None,
                "filename": filename,
                "subfolder": sub,
                "filepath": str(entry),
                "kind": kind,
                "ext": entry.suffix.lower(),
                "size": stat.st_size,
                "mtime": int(stat.st_mtime),
                "width": None,
                "height": None,
                "duration": None,
                "rating": 0,
                "tags": [],
                "has_workflow": None,
                "has_generation_data": None,
                "type": asset_type,
                "root_id": root_id,
            }
        )
    return entries


async def _fs_cache_get_or_build(
    base: Path,
    target_dir_resolved: Path,
    asset_type: str,
    root_id: Optional[str],
) -> Result[dict[str, Any]]:
    # Made async to allow async locking if needed, though mostly sync logic
    try:
        dir_mtime_ns = target_dir_resolved.stat().st_mtime_ns
    except OSError as exc:
        return Result.Err("DIR_NOT_FOUND", sanitize_error_message(exc, "Directory not found"))
    try:
        ensure_fs_list_cache_watching(str(base))
    except Exception:
        pass
    try:
        watch_token = int(get_fs_list_cache_token(str(base)))
    except Exception:
        watch_token = 0

    cache_key = f"{str(base)}|{str(target_dir_resolved)}|{asset_type}|{str(root_id or '')}"
    
    async with _FS_LIST_CACHE_LOCK:
        cached = _FS_LIST_CACHE.get(cache_key)
        if (
            cached
            and cached.get("dir_mtime_ns") == dir_mtime_ns
            and int(cached.get("watch_token") or 0) == watch_token
            and isinstance(cached.get("entries"), list)
        ):
            try:
                now = time.time()
                cached_at = float(cached.get("cached_at") or 0.0)
                if cached_at and (now - cached_at) <= float(FS_LIST_CACHE_TTL_SECONDS):
                    _FS_LIST_CACHE.move_to_end(cache_key)
                    return Result.Ok({"entries": cached["entries"], "dir_mtime_ns": dir_mtime_ns})
            except Exception:
                pass
            _FS_LIST_CACHE.move_to_end(cache_key)

    try:
        entries = await asyncio.to_thread(
            _collect_filesystem_entries,
            target_dir_resolved,
            base,
            asset_type,
            root_id,
        )
    except OSError as exc:
        return Result.Err(
            "LIST_FAILED", sanitize_error_message(exc, "Failed to list directory")
        )

    async with _FS_LIST_CACHE_LOCK:
        # `watch_token` may be implemented later (fs watch integration); keep None for now.
        _FS_LIST_CACHE[cache_key] = {
            "dir_mtime_ns": dir_mtime_ns,
            "watch_token": watch_token,
            "entries": entries,
            "cached_at": time.time(),
        }
        _FS_LIST_CACHE.move_to_end(cache_key)
        while len(_FS_LIST_CACHE) > int(FS_LIST_CACHE_MAX):
            _FS_LIST_CACHE.popitem(last=False)

    return Result.Ok({"entries": entries, "dir_mtime_ns": dir_mtime_ns})


async def _list_filesystem_assets(
    root_dir: Path,
    subfolder: str,
    query: str,
    limit: int,
    offset: int,
    asset_type: str,
    root_id: Optional[str] = None,
    filters: Optional[Mapping[str, Any]] = None,
    index_service: Optional[Any] = None,
    sort: Optional[str] = None,
) -> Result[dict[str, Any]]:
    try:
        base = root_dir.resolve()
    except OSError as exc:
        return Result.Err(
            "INVALID_INPUT", sanitize_error_message(exc, "Invalid root directory")
        )

    rel = _safe_rel_path(subfolder or "")
    if rel is None:
        return Result.Err("INVALID_INPUT", "Invalid subfolder")

    target_dir = (base / rel)
    try:
        target_dir_resolved = target_dir.resolve(strict=True)
    except OSError:
        return Result.Err("DIR_NOT_FOUND", f"Directory not found: {target_dir}")

    if not _is_within_root(target_dir_resolved, base):
        return Result.Err("INVALID_INPUT", "Subfolder outside root")

    q = (query or "*").strip()
    q_lower = q.lower()
    browse_all = q == "*" or q == ""

    filter_kind = str(filters.get("kind") or "").strip().lower() if filters is not None else ""
    filter_min_rating = int(filters.get("min_rating") or 0) if filters is not None else 0
    filter_workflow_only = bool(filters.get("has_workflow")) if filters is not None else False
    filter_extensions = _normalize_extensions(filters.get("extensions") if filters is not None else None)

    mtime_start_raw = filters.get("mtime_start") if filters is not None else None
    mtime_end_raw = filters.get("mtime_end") if filters is not None else None
    filter_mtime_start = int(mtime_start_raw) if mtime_start_raw is not None else None
    filter_mtime_end = int(mtime_end_raw) if mtime_end_raw is not None else None

    # Changed to await the async cache builder
    cache_result = await _fs_cache_get_or_build(base, target_dir_resolved, asset_type, root_id)
    if not cache_result.ok:
        return cache_result
    all_entries = cache_result.data.get("entries") if isinstance(cache_result.data, dict) else None
    if not isinstance(all_entries, list):
        return Result.Err("LIST_FAILED", "Failed to list directory")

    entries = []
    for item in all_entries:
        if not isinstance(item, dict):
            continue
        filename = str(item.get("filename") or "")
        kind = str(item.get("kind") or "")
        if not filename or kind == "unknown":
            continue
        if filter_kind and kind != filter_kind:
            continue
        if not browse_all and q_lower not in filename.lower():
            continue
        entries.append(item)

    sort_key = _normalize_sort_key(sort)
    if sort_key == "name_asc":
        entries.sort(key=lambda x: str(x.get("filename") or "").lower())
    elif sort_key == "name_desc":
        entries.sort(key=lambda x: str(x.get("filename") or "").lower(), reverse=True)
    elif sort_key == "mtime_asc":
        entries.sort(key=lambda x: int(x.get("mtime") or 0))
    else:
        entries.sort(key=lambda x: int(x.get("mtime") or 0), reverse=True)

    # Enrich with DB fields (workflow/generation/tags/rating) when available.
    try:
        lookup = getattr(index_service, "lookup_assets_by_filepaths", None)
        if callable(lookup) and entries:
            filepaths = [str(a.get("filepath") or "") for a in entries if isinstance(a, dict)]
            filepaths = [p for p in filepaths if p]
            if filepaths:
                enrich_result = await lookup(filepaths)
                if enrich_result and getattr(enrich_result, "ok", False) and isinstance(enrich_result.data, dict):
                    mapping = enrich_result.data
                    for asset in entries:
                        if not isinstance(asset, dict):
                            continue
                        fp = str(asset.get("filepath") or "")
                        db_row = mapping.get(fp)
                        if not isinstance(db_row, dict):
                            continue
                        asset["id"] = db_row.get("id")
                        asset["rating"] = int(db_row.get("rating") or 0)
                        asset["tags"] = db_row.get("tags") or []
                        asset["has_workflow"] = db_row.get("has_workflow")
                        asset["has_generation_data"] = db_row.get("has_generation_data")
                        # Prefer stored root_id if present (custom)
                        if db_row.get("root_id"):
                            asset["root_id"] = db_row.get("root_id")
    except Exception as exc:
        logger.debug("Filesystem DB enrichment skipped: %s", exc)

    # Apply filters after enrichment.
    total = 0
    paged: list[dict[str, Any]] = []
    start = max(0, int(offset or 0))
    limit_int = int(limit or 0)
    end = start + limit_int if limit_int > 0 else None

    for item in entries:
        entry_ext = _normalize_extension(item.get("ext"))
        if filter_extensions and entry_ext and entry_ext not in filter_extensions:
            continue
        if filter_kind and item.get("kind") != filter_kind:
            continue
        if filter_min_rating > 0 and int(item.get("rating", 0)) < filter_min_rating:
            continue
        if filter_workflow_only and not _is_truthy_boolish(item.get("has_workflow")):
            continue
        try:
            entry_mtime = int(item.get("mtime") or 0)
        except Exception:
            entry_mtime = 0
        if filter_mtime_start is not None and entry_mtime < filter_mtime_start:
            continue
        if filter_mtime_end is not None and entry_mtime >= filter_mtime_end:
            continue
        if not browse_all and q_lower not in str(item.get("filename", "")).lower():
            continue

        # This entry passed all filters; count it for total.
        if total >= start:
            if end is None or len(paged) < (end - start):
                paged.append(item)
        total += 1

    return Result.Ok({"assets": paged, "total": total, "limit": limit, "offset": offset, "query": q, "sort": sort_key})

